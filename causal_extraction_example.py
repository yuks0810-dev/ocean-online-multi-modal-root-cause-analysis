#!/usr/bin/env python3
"""
OCEANå› æœé–¢ä¿‚æŠ½å‡ºã®å®Ÿè·µçš„ãªä¾‹

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã¯ã€OCEANãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦å®Ÿéš›ã«ãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹é–“ã®
å› æœé–¢ä¿‚ã‚’æŠ½å‡ºã—ã€æ ¹æœ¬åŸå› åˆ†æã‚’è¡Œã†æ–¹æ³•ã‚’ç¤ºã—ã¾ã™ã€‚
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from typing import Dict, List, Tuple, Optional
from datetime import datetime, timedelta

from ocean.configs.default_config import default_config
from ocean.models.ocean_model import OCEANModel
from ocean.data.data_types import ServiceGraph, DatasetSample
from ocean.evaluation.metrics import PerformanceMetrics


class CausalRelationshipExtractor:
    """
    å› æœé–¢ä¿‚æŠ½å‡ºã®ãŸã‚ã®ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚¯ãƒ©ã‚¹
    
    OCEANãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã‹ã‚‰å› æœé–¢ä¿‚ã‚’è§£é‡ˆã—ã€
    æ ¹æœ¬åŸå› åˆ†æã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
    """
    
    def __init__(self, model: OCEANModel, service_names: List[str]):
        self.model = model
        self.service_names = service_names
        self.num_services = len(service_names)
        
    def extract_causal_relationships(self, 
                                   metrics: torch.Tensor,
                                   service_graph: ServiceGraph,
                                   logs: Optional[torch.Tensor] = None,
                                   threshold: float = 0.5) -> Dict:
        """
        å› æœé–¢ä¿‚ã‚’æŠ½å‡ºã—ã€æ ¹æœ¬åŸå› ã‚’ç‰¹å®š
        
        Args:
            metrics: æ™‚ç³»åˆ—ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ‡ãƒ¼ã‚¿ (batch, seq_len, features)
            service_graph: ã‚µãƒ¼ãƒ“ã‚¹ä¾å­˜ã‚°ãƒ©ãƒ•
            logs: ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ (batch, seq_len, log_dim)
            threshold: æ ¹æœ¬åŸå› ã®ç¢ºä¿¡åº¦é–¾å€¤
            
        Returns:
            å› æœé–¢ä¿‚åˆ†æçµæœã‚’å«ã‚€è¾æ›¸
        """
        self.model.eval()
        
        with torch.no_grad():
            # ä¸­é–“è¡¨ç¾ã‚‚å–å¾—ã™ã‚‹ãŸã‚ã®ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹
            # ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
            if logs is None:
                # é©å½“ãªãƒ­ã‚°ç‰¹å¾´é‡ã‚’ç”Ÿæˆï¼ˆå®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯å®Ÿãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼‰
                batch_size, seq_len = metrics.shape[:2]
                logs = torch.randn(batch_size, seq_len, 768) * 0.1  # BERTæ¬¡å…ƒ
            
            outputs = self.model(
                metrics, 
                service_graph, 
                logs, 
                return_intermediate=True
            )
            
            # æ ¹æœ¬åŸå› ç¢ºç‡
            root_cause_probs = outputs['root_cause_probs'].cpu().numpy()
            
            # ä¸­é–“è¡¨ç¾ã‹ã‚‰å› æœé–¢ä¿‚ã‚’æŠ½å‡º
            intermediate = outputs['intermediate']
            
            # æ³¨æ„é‡ã¿ã‹ã‚‰å› æœé–¢ä¿‚ã‚’åˆ†æ
            causal_analysis = self._analyze_attention_weights(
                intermediate, root_cause_probs, threshold
            )
            
            # ã‚µãƒ¼ãƒ“ã‚¹é–“ã®å½±éŸ¿åº¦ã‚’è¨ˆç®—
            service_influences = self._compute_service_influences(
                intermediate, service_graph
            )
            
            # æ™‚ç³»åˆ—çš„ãªå› æœé–¢ä¿‚ã‚’æŠ½å‡º
            temporal_causality = self._extract_temporal_causality(
                intermediate, metrics
            )
            
        return {
            'root_cause_probabilities': root_cause_probs,
            'causal_relationships': causal_analysis,
            'service_influences': service_influences,
            'temporal_causality': temporal_causality,
            'summary': self._generate_causality_summary(
                root_cause_probs, causal_analysis, threshold
            )
        }
    
    def _analyze_attention_weights(self, 
                                 intermediate: Dict,
                                 root_cause_probs: np.ndarray,
                                 threshold: float) -> Dict:
        """æ³¨æ„é‡ã¿ã‹ã‚‰å› æœé–¢ä¿‚ã‚’åˆ†æ"""
        
        # Multi-factor Attentionã®é‡ã¿ã‚’å–å¾—
        attention_details = intermediate.get('fusion_details', {})
        
        causal_relationships = {
            'cross_modal_attention': {},
            'temporal_dependencies': {},
            'spatial_relationships': {}
        }
        
        # ã‚¯ãƒ­ã‚¹ãƒ¢ãƒ¼ãƒ€ãƒ«æ³¨æ„é‡ã¿ã®åˆ†æ
        if 'attention_weights' in attention_details:
            attention_weights = attention_details['attention_weights']
            
            # å„ã‚µãƒ¼ãƒ“ã‚¹ã«å¯¾ã™ã‚‹æ³¨æ„é‡ã¿ã‚’åˆ†æ
            for i, service in enumerate(self.service_names):
                if root_cause_probs[0, i] > threshold:
                    # é«˜ã„æ ¹æœ¬åŸå› ç¢ºç‡ã‚’æŒã¤ã‚µãƒ¼ãƒ“ã‚¹ã®æ³¨æ„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ
                    service_attention = attention_weights[0, :, i]  # (seq_len,)
                    
                    causal_relationships['cross_modal_attention'][service] = {
                        'attention_pattern': service_attention.cpu().numpy().tolist(),
                        'peak_attention_time': int(torch.argmax(service_attention)),
                        'average_attention': float(torch.mean(service_attention))
                    }
        
        return causal_relationships
    
    def _compute_service_influences(self, 
                                  intermediate: Dict,
                                  service_graph: ServiceGraph) -> Dict:
        """ã‚µãƒ¼ãƒ“ã‚¹é–“ã®å½±éŸ¿åº¦ã‚’è¨ˆç®—"""
        
        # ã‚°ãƒ©ãƒ•ç‰¹å¾´é‡ã‹ã‚‰å½±éŸ¿åº¦ã‚’è¨ˆç®—
        spatial_features = intermediate['spatial_features']  # (batch, num_services, features)
        
        # ã‚µãƒ¼ãƒ“ã‚¹é–“ã®é¡ä¼¼åº¦è¡Œåˆ—ã‚’è¨ˆç®—
        # spatial_featuresã¯(batch_size, spatial_dim)å½¢çŠ¶ãªã®ã§ã€ã‚µãƒ¼ãƒ“ã‚¹æ¯ã«åˆ†å‰²ã™ã‚‹å¿…è¦ãŒã‚ã‚‹
        # ç°¡ç•¥åŒ–: å…¨ã‚µãƒ¼ãƒ“ã‚¹ã«åŒã˜ç‰¹å¾´é‡ã‚’é©ç”¨
        spatial_feature = spatial_features[0]  # (spatial_dim,)
        num_services = len(self.service_names)
        
        # å„ã‚µãƒ¼ãƒ“ã‚¹ã«åŒã˜ç‰¹å¾´é‡ã‚’è¤‡è£½
        service_features = spatial_feature.unsqueeze(0).repeat(num_services, 1)  # (num_services, spatial_dim)
        
        # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—
        similarity_matrix = torch.cosine_similarity(
            service_features.unsqueeze(1),  # (num_services, 1, spatial_dim)
            service_features.unsqueeze(0),  # (1, num_services, spatial_dim)
            dim=2  # spatial_dimæ¬¡å…ƒã§è¨ˆç®—
        )
        
        # éš£æ¥è¡Œåˆ—ã¨ã®çµ„ã¿åˆã‚ã›ã§å®Ÿéš›ã®å½±éŸ¿åº¦ã‚’è¨ˆç®—
        adjacency = service_graph.adjacency_matrix
        influence_matrix = similarity_matrix * adjacency
        
        # å„ã‚µãƒ¼ãƒ“ã‚¹ã®å½±éŸ¿åº¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        service_influences = {}
        for i, service in enumerate(self.service_names):
            # ãã®ã‚µãƒ¼ãƒ“ã‚¹ãŒä»–ã®ã‚µãƒ¼ãƒ“ã‚¹ã«ä¸ãˆã‚‹å½±éŸ¿
            outgoing_influence = float(torch.sum(influence_matrix[i, :]))
            # ãã®ã‚µãƒ¼ãƒ“ã‚¹ãŒä»–ã®ã‚µãƒ¼ãƒ“ã‚¹ã‹ã‚‰å—ã‘ã‚‹å½±éŸ¿
            incoming_influence = float(torch.sum(influence_matrix[:, i]))
            
            service_influences[service] = {
                'outgoing_influence': outgoing_influence,
                'incoming_influence': incoming_influence,
                'influence_ratio': outgoing_influence / (incoming_influence + 1e-8),
                'connected_services': [
                    self.service_names[j] for j in range(self.num_services)
                    if adjacency[i, j] > 0 and i != j
                ]
            }
        
        return service_influences
    
    def _extract_temporal_causality(self, 
                                  intermediate: Dict,
                                  metrics: torch.Tensor) -> Dict:
        """æ™‚ç³»åˆ—çš„ãªå› æœé–¢ä¿‚ã‚’æŠ½å‡º"""
        
        temporal_features = intermediate['temporal_features']  # (batch, seq_len, features)
        seq_len = temporal_features.shape[1]
        
        # æ™‚åˆ»é–“ã®å› æœé–¢ä¿‚ã‚’åˆ†æ
        temporal_causality = {
            'lag_analysis': {},
            'change_points': [],
            'trend_analysis': {}
        }
        
        # å„ã‚µãƒ¼ãƒ“ã‚¹ã®æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ã‚µãƒ¼ãƒ“ã‚¹æ•°ã§åˆ†å‰²ï¼ˆ12æ¬¡å…ƒã‚’5ã‚µãƒ¼ãƒ“ã‚¹ã§åˆ†å‰²ã§ããªã„ãŸã‚ã€é©åˆ‡ã«å‡¦ç†ï¼‰
        metrics_per_service = metrics.shape[-1] // len(self.service_names)  # 12 // 5 = 2
        
        for i, service in enumerate(self.service_names):
            # å„ã‚µãƒ¼ãƒ“ã‚¹ã«å¯¾ã—ã¦2-3æ¬¡å…ƒã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å‰²ã‚Šå½“ã¦
            start_idx = i * metrics_per_service
            end_idx = min((i + 1) * metrics_per_service, metrics.shape[-1])
            if i == len(self.service_names) - 1:  # æœ€å¾Œã®ã‚µãƒ¼ãƒ“ã‚¹ã¯æ®‹ã‚Šã‚’å…¨ã¦ä½¿ç”¨
                end_idx = metrics.shape[-1]
                
            service_metrics = metrics[0, :, start_idx:end_idx]
            
            # å¤‰åŒ–ç‚¹ã®æ¤œå‡ºï¼ˆç°¡å˜ãªå·®åˆ†ãƒ™ãƒ¼ã‚¹ï¼‰
            diff = torch.diff(torch.mean(service_metrics, dim=1))
            change_indices = torch.where(torch.abs(diff) > torch.std(diff) * 2)[0]
            
            temporal_causality['change_points'].extend([
                {
                    'service': service,
                    'time_index': int(idx),
                    'magnitude': float(diff[idx])
                }
                for idx in change_indices
            ])
            
            # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
            service_avg = torch.mean(service_metrics, dim=1).cpu().numpy()
            x_range = range(len(service_avg))  # service_metricsã®å®Ÿéš›ã®é•·ã•ã‚’ä½¿ç”¨
            trend = np.polyfit(x_range, service_avg, 1)[0]
            temporal_causality['trend_analysis'][service] = {
                'trend_slope': float(trend),
                'trend_direction': 'increasing' if trend > 0 else 'decreasing'
            }
        
        return temporal_causality
    
    def _generate_causality_summary(self, 
                                  root_cause_probs: np.ndarray,
                                  causal_analysis: Dict,
                                  threshold: float) -> Dict:
        """å› æœé–¢ä¿‚åˆ†æã®è¦ç´„ã‚’ç”Ÿæˆ"""
        
        # æ ¹æœ¬åŸå› ã®å€™è£œã‚’ç‰¹å®š
        likely_root_causes = []
        for i, service in enumerate(self.service_names):
            prob = root_cause_probs[0, i]
            if prob > threshold:
                likely_root_causes.append({
                    'service': service,
                    'probability': float(prob),
                    'confidence': 'high' if prob > 0.8 else 'medium'
                })
        
        # ç¢ºç‡é †ã«ã‚½ãƒ¼ãƒˆ
        likely_root_causes.sort(key=lambda x: x['probability'], reverse=True)
        
        return {
            'likely_root_causes': likely_root_causes,
            'top_root_cause': likely_root_causes[0] if likely_root_causes else None,
            'num_potential_causes': len(likely_root_causes),
            'analysis_timestamp': datetime.now().isoformat()
        }
    
    def visualize_causal_relationships(self, 
                                     causal_results: Dict,
                                     save_path: Optional[str] = None):
        """å› æœé–¢ä¿‚ã®å¯è¦–åŒ–"""
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. æ ¹æœ¬åŸå› ç¢ºç‡ã®ãƒãƒ¼ãƒ—ãƒ­ãƒƒãƒˆ
        probs = causal_results['root_cause_probabilities'][0]
        ax1.bar(self.service_names, probs)
        ax1.set_title('Root Cause Probabilities')
        ax1.set_ylabel('Probability')
        ax1.tick_params(axis='x', rotation=45)
        
        # 2. ã‚µãƒ¼ãƒ“ã‚¹å½±éŸ¿åº¦ãƒãƒˆãƒªãƒƒã‚¯ã‚¹
        influences = causal_results['service_influences']
        influence_matrix = np.zeros((self.num_services, self.num_services))
        
        for i, service in enumerate(self.service_names):
            if service in influences:
                influence_matrix[i, i] = influences[service]['outgoing_influence']
        
        im2 = ax2.imshow(influence_matrix, cmap='RdYlBu_r')
        ax2.set_title('Service Influence Matrix')
        ax2.set_xticks(range(self.num_services))
        ax2.set_yticks(range(self.num_services))
        ax2.set_xticklabels(self.service_names, rotation=45)
        ax2.set_yticklabels(self.service_names)
        plt.colorbar(im2, ax=ax2)
        
        # 3. æ™‚ç³»åˆ—ãƒˆãƒ¬ãƒ³ãƒ‰
        trends = causal_results['temporal_causality']['trend_analysis']
        trend_slopes = [trends[service]['trend_slope'] for service in self.service_names]
        
        colors = ['red' if slope > 0 else 'blue' for slope in trend_slopes]
        ax3.bar(self.service_names, trend_slopes, color=colors)
        ax3.set_title('Service Trend Analysis')
        ax3.set_ylabel('Trend Slope')
        ax3.tick_params(axis='x', rotation=45)
        
        # 4. è¦ç´„ãƒ†ã‚­ã‚¹ãƒˆ
        summary = causal_results['summary']
        
        # ç¢ºä¿¡åº¦ã®æ–‡å­—åˆ—ã‚’äº‹å‰ã«æº–å‚™
        if summary['top_root_cause']:
            service_name = summary['top_root_cause']['service']
            confidence = f"{summary['top_root_cause']['probability']:.3f}"
        else:
            service_name = 'None'
            confidence = 'N/A'
        
        summary_text = f"""
        åˆ†æçµæœè¦ç´„:
        
        æœ€æœ‰åŠ›æ ¹æœ¬åŸå› : {service_name}
        ç¢ºä¿¡åº¦: {confidence}
        
        å€™è£œæ•°: {summary['num_potential_causes']}
        
        ä¸Šä½3å€™è£œ:
        """
        
        for i, cause in enumerate(summary['likely_root_causes'][:3]):
            summary_text += f"\n{i+1}. {cause['service']}: {cause['probability']:.3f}"
        
        ax4.text(0.1, 0.5, summary_text, transform=ax4.transAxes, 
                fontsize=10, verticalalignment='center')
        ax4.set_title('Analysis Summary')
        ax4.axis('off')
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        
        plt.show()


def create_sample_scenario():
    """ã‚µãƒ³ãƒ—ãƒ«ã‚·ãƒŠãƒªã‚ªã®ä½œæˆ"""
    
    # 5ã¤ã®ãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹æ§‹æˆ
    service_names = ['web-frontend', 'api-gateway', 'user-service', 'database', 'cache']
    num_services = len(service_names)
    
    # ã‚µãƒ¼ãƒ“ã‚¹ä¾å­˜ã‚°ãƒ©ãƒ•ã®ä½œæˆ
    # web -> api -> user-service -> database
    #              api -> cache
    adjacency_matrix = torch.zeros(num_services, num_services)
    adjacency_matrix[0, 1] = 1  # web -> api
    adjacency_matrix[1, 2] = 1  # api -> user-service
    adjacency_matrix[2, 3] = 1  # user-service -> database
    adjacency_matrix[1, 4] = 1  # api -> cache
    
    # ãƒãƒ¼ãƒ‰ç‰¹å¾´é‡ï¼ˆå„ã‚µãƒ¼ãƒ“ã‚¹ã®åŸºæœ¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼‰
    node_features = torch.randn(num_services, 12)  # 12æ¬¡å…ƒã®ç‰¹å¾´é‡
    
    service_graph = ServiceGraph(
        adjacency_matrix=adjacency_matrix,
        node_features=node_features,
        service_names=service_names
    )
    
    # ç•°å¸¸ã‚·ãƒŠãƒªã‚ªã®æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
    seq_len = 20
    batch_size = 1
    # DCNNãŒæœŸå¾…ã™ã‚‹12æ¬¡å…ƒã®å…¥åŠ›ç‰¹å¾´é‡ã«åˆã‚ã›ã‚‹
    input_features = 12
    
    # æ­£å¸¸æ™‚ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³
    baseline_metrics = torch.randn(batch_size, seq_len, input_features) * 0.1
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã§ç•°å¸¸ãŒç™ºç”Ÿã—ã€ãã‚ŒãŒä»–ã®ã‚µãƒ¼ãƒ“ã‚¹ã«æ³¢åŠã™ã‚‹ã‚·ãƒŠãƒªã‚ª
    # æ™‚åˆ»10ã‹ã‚‰ç•°å¸¸ãŒé–‹å§‹
    anomaly_start = 10
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹å…¨ä½“ã«ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¿½åŠ 
    # æœ€åˆã®3æ¬¡å…ƒã‚’CPUä½¿ç”¨ç‡ã€æ¬¡ã®3æ¬¡å…ƒã‚’ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ã€æ¬¡ã®3æ¬¡å…ƒã‚’ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã€æ®‹ã‚Šã‚’ãã®ä»–ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¨ã™ã‚‹
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ç•°å¸¸ï¼ˆãƒ¡ãƒˆãƒªã‚¯ã‚¹9-11ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹é–¢é€£ã¨ã™ã‚‹ï¼‰
    baseline_metrics[0, anomaly_start:, 9:12] += 2.0
    
    # user-serviceã¸ã®å½±éŸ¿ï¼ˆãƒ¡ãƒˆãƒªã‚¯ã‚¹6-8ã‚’é–¢é€£ã¨ã™ã‚‹ï¼‰
    baseline_metrics[0, anomaly_start+2:, 6:9] += 1.5
    
    # api-gatewayã¸ã®å½±éŸ¿ï¼ˆãƒ¡ãƒˆãƒªã‚¯ã‚¹3-5ã‚’é–¢é€£ã¨ã™ã‚‹ï¼‰
    baseline_metrics[0, anomaly_start+4:, 3:6] += 1.0
    
    # web-frontendã¸ã®å½±éŸ¿ï¼ˆãƒ¡ãƒˆãƒªã‚¯ã‚¹0-2ã‚’é–¢é€£ã¨ã™ã‚‹ï¼‰
    baseline_metrics[0, anomaly_start+6:, 0:3] += 0.5
    
    return service_graph, baseline_metrics, service_names


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    
    print("ğŸ” OCEANå› æœé–¢ä¿‚æŠ½å‡ºã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")
    print("=" * 50)
    
    # 1. ã‚µãƒ³ãƒ—ãƒ«ã‚·ãƒŠãƒªã‚ªã®ä½œæˆ
    print("ğŸ“Š ã‚µãƒ³ãƒ—ãƒ«ã‚·ãƒŠãƒªã‚ªã‚’ä½œæˆä¸­...")
    service_graph, metrics, service_names = create_sample_scenario()
    
    # 2. OCEANãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
    print("ğŸ¤– OCEANãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ä¸­...")
    config = default_config()
    # ã‚µãƒ³ãƒ—ãƒ«ã‚·ãƒŠãƒªã‚ªã«åˆã‚ã›ã¦è¨­å®šã‚’èª¿æ•´
    config.model.num_services = len(service_names)
    config.data.sequence_length = 20
    # å…¨ã¦ã®æ¬¡å…ƒã‚’çµ±ä¸€ã—ã¦32ã«è¨­å®š
    config.model.gnn_hidden_dim = 32
    config.model.hidden_dim = 32
    config.model.attention_dim = 32
    config.model.temporal_dim = 32
    config.model.spatial_dim = 32
    try:
        model = OCEANModel(config)
        model.eval()
        print("âœ… ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ã§ã‚¨ãƒ©ãƒ¼: {e}")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚ˆã‚Šå°ã•ãªè¨­å®šã‚’è©¦ã™
        config.model.gnn_hidden_dim = 16
        config.model.hidden_dim = 16
        config.model.attention_dim = 16
        config.model.temporal_dim = 16
        config.model.spatial_dim = 16
        model = OCEANModel(config)
        model.eval()
        print("âœ… ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®šã§ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–æˆåŠŸ")
    
    # 3. å› æœé–¢ä¿‚æŠ½å‡ºå™¨ã®åˆæœŸåŒ–
    print("ğŸ”¬ å› æœé–¢ä¿‚æŠ½å‡ºå™¨ã‚’åˆæœŸåŒ–ä¸­...")
    extractor = CausalRelationshipExtractor(model, service_names)
    
    # 4. å› æœé–¢ä¿‚ã®æŠ½å‡º
    print("ğŸ¯ å› æœé–¢ä¿‚ã‚’æŠ½å‡ºä¸­...")
    causal_results = extractor.extract_causal_relationships(
        metrics=metrics,
        service_graph=service_graph,
        threshold=0.3  # é–¾å€¤ã‚’ä½ãè¨­å®š
    )
    
    # 5. çµæœã®è¡¨ç¤º
    print("\nğŸ“‹ åˆ†æçµæœ:")
    print("-" * 30)
    
    summary = causal_results['summary']
    print(f"æœ€æœ‰åŠ›æ ¹æœ¬åŸå› : {summary['top_root_cause']['service'] if summary['top_root_cause'] else 'None'}")
    
    if summary['top_root_cause']:
        print(f"ç¢ºä¿¡åº¦: {summary['top_root_cause']['probability']:.3f}")
    
    print(f"\næ ¹æœ¬åŸå› å€™è£œæ•°: {summary['num_potential_causes']}")
    
    print("\nä¸Šä½æ ¹æœ¬åŸå› å€™è£œ:")
    for i, cause in enumerate(summary['likely_root_causes'][:3]):
        print(f"  {i+1}. {cause['service']}: {cause['probability']:.3f} ({cause['confidence']})")
    
    # 6. ã‚µãƒ¼ãƒ“ã‚¹å½±éŸ¿åº¦ã®è¡¨ç¤º
    print("\nğŸŒ ã‚µãƒ¼ãƒ“ã‚¹å½±éŸ¿åº¦åˆ†æ:")
    influences = causal_results['service_influences']
    for service, influence in influences.items():
        print(f"  {service}:")
        print(f"    å¤–å‘ãå½±éŸ¿: {influence['outgoing_influence']:.3f}")
        print(f"    å†…å‘ãå½±éŸ¿: {influence['incoming_influence']:.3f}")
        print(f"    å½±éŸ¿æ¯”ç‡: {influence['influence_ratio']:.3f}")
    
    # 7. æ™‚ç³»åˆ—å› æœé–¢ä¿‚ã®è¡¨ç¤º
    print("\nâ° æ™‚ç³»åˆ—å› æœé–¢ä¿‚åˆ†æ:")
    temporal = causal_results['temporal_causality']
    
    print("  å¤‰åŒ–ç‚¹:")
    for change in temporal['change_points'][:5]:  # ä¸Šä½5ã¤
        print(f"    {change['service']} (æ™‚åˆ»{change['time_index']}): {change['magnitude']:.3f}")
    
    print("\n  ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ:")
    for service, trend in temporal['trend_analysis'].items():
        direction = "â†—ï¸" if trend['trend_direction'] == 'increasing' else "â†˜ï¸"
        print(f"    {service}: {direction} {trend['trend_slope']:.3f}")
    
    # 8. å¯è¦–åŒ–
    print("\nğŸ“ˆ å› æœé–¢ä¿‚ã‚’å¯è¦–åŒ–ä¸­...")
    try:
        extractor.visualize_causal_relationships(
            causal_results, 
            save_path="causal_analysis_results.png"
        )
        print("âœ… å¯è¦–åŒ–çµæœã‚’ 'causal_analysis_results.png' ã«ä¿å­˜ã—ã¾ã—ãŸ")
    except Exception as e:
        print(f"âš ï¸ å¯è¦–åŒ–ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    
    print("\nğŸ‰ å› æœé–¢ä¿‚æŠ½å‡ºãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Œäº†!")
    
    return causal_results


if __name__ == "__main__":
    main()